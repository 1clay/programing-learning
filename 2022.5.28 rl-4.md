# 2022.5.28 rl-4

### 交叉熵方法

##### 1.交叉熵方法的实践部分。

##### 2.交叉熵方法在两个Gym环境（熟悉的CartPole和FrozenLake网格世界）的应用。

##### 3.交叉熵方法的理论背景。

#### 4.1　RL方法的分类

▪  无模型或基于模型。
▪  基于价值或基于策略。
▪  在线策略（on-policy）或离线策略（off-policy）。

交叉熵方法是无模型的、基于策略的在线策略的方法：
▪  它不构建环境的任何模型，只告诉智能体每一步需要做什么。
▪  它计算智能体的策略。
▪  它从环境中获取新数据。

#### 4.2　交叉熵方法的实践

**交叉熵方法的描述可以分成两个不同的方面，实践方面是这个方法的直观表示，而理论方面解释了为什么交叉熵方法有用，以及它如何生效，这会更复杂。**

交叉熵方法的核心将差的片段丢掉，用好的片段来训练，步骤如下：

1）使用当前的模型和环境产生N次片段。
2）计算每个片段的总奖励，并确定奖励边界。通常使用总奖励的百分位来确定，例如50或70。
3）将奖励在边界之下的片段丢掉。
4）用观察值作为输入、智能体产生的动作作为目标输出，训练剩余的“精英”片段。
5）从第1步开始重复，直到得到满意的结果。

#### 4.3　交叉熵方法在CartPole中的应用

我们定义了两个命名元组类型的帮助类，来自标准库中的collections包：
▪  EpisodeStep：这会用于表示智能体在片段中执行的一步，同时它会保存来自环境的观察以及智能体采取了什么动作。在“精英”片段的训练中会用到它们。
▪  Episode：这是单个片段，它保存了总的无折扣奖励以及EpisodeStep集合。

在每次迭代中，将当前的观察转换成PyTorch张量，并将其传入NN以获得动作概率分布，需要注意：
▪  所有PyTorch中的nn.Module实例都接受一批数据，对于NN也是一样的，所以我们将观察（在CartPole中为一个由4个数字组成的向量）转换成1×4大小的张量（为此，将观察放入单元素的列表中）。
▪  由于没有在NN的输出使用非线性函数，它会输出一个原始的动作分数，因此需要将其用softmax函数处理。
▪  NN和softmax层都返回包含了梯度的张量，所以我们需要通过访问tensor.data字段来将其数据取出来，然后将张量转换成NumPy数组。该数组和输入一样，有同样的二维结构，0轴是批的维度，所以我们需要获取第一个元素，这样才能得到动作概率的一维向量。

#### 4.4　交叉熵方法在FrozenLake中的应用

交叉熵方法存在的限制：
▪  对于训练来说，片段必须是有限的、优秀的、简短的。
▪  片段的总奖励应该有足够的差异来区分好的片段和差的片段。
▪  没有中间值来表明智能体成功了还是失败了。

就目前而言：
▪  每批包含更多的片段：在CartPole中，每个迭代只用16个片段就足够了，但是FrozenLake需要起码100个片段，才能获得一些成功的片段。
▪  对奖励使用折扣系数：为了让总奖励能考虑片段的长度，并增加片段的多样性，可以使用折扣因子是0.9或0.95的折扣总奖励。在这种情况下，较短的片段将比那些较长的片段得到更高的奖励。这将增加奖励分布的多样性，有助于避开图4.10所描述的情况。
▪  让“精英”片段保持更长的时间：在CartPole训练中，我们从环境中采样片段，训练最好的一些片段，然后将它们丢掉。在FrozenLake环境中，成功的片段十分珍贵，所以需要将它们保留并训练好几次迭代。
▪  降低学习率：这给NN机会来平均更多的训练样本。
▪  更长的训练时间：由于成功片段的稀有性以及动作结果的随机性，NN会更难决定在某个特定场景下应该执行什么动作。要将片段的成功率提升至50%，起码需要训练5000次迭代。

#### 4.5　交叉熵方法的理论背景

交叉熵方法，尽管它有局限性，但简单且功能强大，并将其应用在了CartPole环境（取得了巨大的成功）和FrozenLake环境（效果还行）。另外，还讨论了RL方法的分类，这会在本书的其余部分多次引用，因为解决RL问题的不同方法会有不同特性，从而影响了它们的适用性。