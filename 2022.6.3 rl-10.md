# 2022.6.3 rl-10

### 使用强化学习进行股票交易

▪  实现自己的OpenAI Gym环境来模拟股票市场。
▪  用第6章和第8章中的DQN方法来训练智能体进行股票交易以最大化利润

问题是：是否可以从RL角度看待这个问题？假设我们对市场有一些观察，并想做出一个决定：购买、出售或等待。如果在价格上涨之前买入，利润将为正；否则，将获得负奖励。我们要尝试的是获得尽可能多的利润。这样市场交易和RL之间的联系就非常明显了。

**观察将包括以下信息：**
▪  N个过去的bar，每个都有开盘价、最高价、最低价和收盘价。
▪  表明该股票是在一段时间前购买的标识（同一时间只可能有一份股票被购买）。
▪  根据当前价位（所买股票）计算的收益或损失。
智能体每一步（每个分钟bar）可以执行下面动作之一：
▪  什么也不做：跳过此bar，不执行任何动作。
▪  买入一支股票：如果智能体已经持有，则不会再购买；否则，支付佣金（通常是当前价格的一个小百分比）。
▪  平仓：如果之前没有购买股票，则不会发生任何事情；否则，支付交易佣金。

**环境的构造函数接受许多参数以调整环境的行为和观察表示**：
▪  prices：作为字典包含一个或多个机构的一支或多支股票价格，其中键是机构的名称，值是容器对象data.Prices，保存价格数据数组。
▪  bars_count：在观察中经历的bar，默认情况下，为10个bar。
▪  commission：在买卖股票时必须支付给经纪人的股票价格的百分比，默认情况下为0.1%。
▪  reset_on_close：如果将此参数设置为True（默认情况下为True），则每当智能体要求平仓（即出售股票）时，都会停止该片段。否则，片段将持续到时间序列结束为止，也就是一年的数据。
▪  conv_1d：传递给智能体的观察中的价格数据的不同表示形式，能够通过此布尔参数进行切换。如果将其设置为True，则观察具有2D形状，bar中不同的价格和其后bar的同类型价格会被组织在同一行中。例如，最高价（bar中的最高价格）放在第一行，第二行是最低价，第三行是收盘价。此表示形式适用于在时间序列上进行一维卷积，其中数据中的每一行与Atari 2D图像中的不同颜色平面（红色、绿色或蓝色）有相同的含义。如果将此选项设置为False，则只有一个数据数组，每个bar的组件都放置在一起。该组织形式对于全连接的网络架构很方便。

▪  random_ofs_on_reset：如果参数为True（默认情况），则在每次环境重置时，将从时间序列的随机偏移开始。否则，将从数据的开头开始。
▪  reward_on_close：此布尔参数在前面讨论的两种奖励方案之间切换。如果将其设置为True，则智能体将仅在平仓动作产生时获得奖励。否则，将在每个bar给一个小额奖励，与该bar期间的价格变动相对应。
▪  volumes：此参数决定是否在观察中增加交易量，默认情况下处于禁用状态。

**使用RL创建完整且可以获利的交易策略是一个大型项目，可能需要专攻数月。但是，有些事情我们可以尝试一下，以更好地理解该主题：**
▪  我们的数据表示绝对不完美。没有考虑重要的价格水平（支撑位和阻力位）、整数价格值和其他因素。将它们纳入观察可能是一个具有挑战性的问题。
▪  通常需要针对多个不同时间范围来分析市场价格。像一分钟的bar这样的小范围数据比较嘈杂（因为它们包含许多由单个交易引起的小幅价格浮动），这就像使用显微镜观察市场一样。在较大的范围（例如一小时或一天的bar）下，可以看到数据浮动的长期趋势，这对于价格预测而言可能极为重要。
▪  需要更多的训练数据。一支股票一年的数据只有13万个bar，可能不足以捕捉所有市场情况。理想情况下，应该在更大的数据集上训练更实用的智能体，例如采用过去10年中数百支股票的价格来训练。
▪  实验更多网络架构。卷积模型的收敛速度比前馈模型快得多，但是还有很多要优化的方面：层数、内核大小、残差网络架构及注意力机制等。