# 2022.6.2 rl-9

### 加速强化学习训练的方法

▪  使用第8章的Pong环境，并试图尽可能快地解决它。
▪  使用完全相同的硬件，逐步解决Pong问题并将速度提升3.5倍。
▪  讨论更先进的方法来加速强化学习（RL）训练，这些方法在将来可能会很常见。

**即使是简单的ML问题，也几乎不可能在第一次尝试时就能正确实现。**
在找到正确的超参、修复所有的bug并让代码就绪前，需要试验很多次。物理模拟、RL研究、大数据处理和一般编程都有同样的过程。所以，如果能让程序运行得更快，并不只是单次程序运行会受益，我们同样可以快速代码迭代并做更多次的实验，这很大程度上能加速整个处理过程并提升最终结果的质量。

**在训练时，代码会向TensorBoard写入几个指标：**
▪  reward：从片段得到的未经折扣的奖励，x轴是片段数。
▪  avg_reward：和奖励一样，只不过用alpha=0.98做了求平均值的平滑处理。
▪  steps：片段持续的步数。通常，一开始智能体很快就输了，所以每个片段大概在1000步左右。然后，它学会如何表现得更出色，所以步数会随着奖励一起增加。但是，在最后，当智能体已经能掌控游戏的时候，步数又落回2000步，因为策略的完善标准是尽快赢得游戏（因为折扣因子γ）。实际上，这种片段长度的降低可能标识着对环境过拟合，这是RL中的一个巨大问题。但是，它不在本书的讨论范围内。
▪  loss：训练时，每迭代100次采样一次的损失。它应该在2e-3到7e-3，当智能体发现新行为时，会导致其奖励值和从Q值中学到的不一样，所以loss偶尔会增加。
▪  avg_loss：平滑版本的损失。
▪  epsilon：当前epsilon值。
▪  avg_fps：智能体和环境交互的速度（每秒的观察数），通过取平均使其更平滑。

**在DQN中，神经网络（NN）通常用于三种场景：**
1）想要用网络预测出来的Q值，对比Bellman方程估计出来的Q值，来获得损失时。
2）使用目标神经网络获得下一个状态的Q值来计算Bellman近似值时。
3）智能体决定执行哪个动作时。

**从宏观角度来看，训练包含下面几个重复步骤：**
1）让当前神经网络选择动作，并在环境数组中执行。
2）将观察放入回放缓冲区。
3）从回放缓冲区中随机采样用于训练的批。
4）训练批。

**通常应用于Atari游戏的DeepMind风格的包装器组是这样的：**
1）NoopResetEnv：在游戏重置的时候应用随机数量的NOOP操作。在某些Atari游戏中，用它可以跳过一些奇怪的初始观察。
2）MaxAndSkipEnv：对N个观察（通常是4个）应用max函数并将其作为一步的观察返回。这可以解决某些Atari游戏中的“闪烁”问题，这些游戏会在屏幕的奇数帧和偶数帧画出画面的不同部分（这是2600开发者公认的增加游戏复杂度的实践）。
3）EpisodicLifeEnv：在某些游戏中，它会发现角色少了一条命并将其视为片段的结束。因为片段更短了（一条命对比游戏给出的多条命），能明显加速收敛。只和Atari 2600学习环境提供的某些游戏相关。
4）FireResetEnv：在游戏重置的时候执行FIRE动作。一些游戏需要它来启动。如果没有它，环境会成为部分可观察的马尔可夫决策过程（POMDP），导致完全没法收敛。
5）WarpFrame也被称为ProcessFrame84，它将图像转换成灰度表示，并将大小调整为84×84。
6）ClipRewardEnv：将奖励裁剪至–1～1，虽然不是最好的，但是也算是个可行方案，可以处理不同Atari游戏中的各种分数。例如，Pong的分数范围可能是–21～21，但是River Raid游戏的分数范围可能是0～∞。
7）FrameStack：将N个（默认是4个）连续观察叠加到一起。如第6章所述，在某些游戏中用它来达成马尔可夫性质。例如，在Pong中，单帧是不可能判断出球的运动方向的。