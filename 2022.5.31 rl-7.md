# 2022.5.31 rl-7

## 高级强化学习库

▪  使用高级库的动机，不要从头开始重新实现一切。
▪  PTAN库以及最重要的部分，将通过代码示例进行说明。
▪  针对CartPole上的DQN，使用PTAN库来实现。
▪  可以考虑的其他RL库。

### 为什么使用强化学习库

RL十分灵活，并且很多现实生活中的问题都属于环境–智能体交互的类型。RL方法不会对观察和动作的细节做很多假设，所以用来解决CartPole环境的代码也适用于Atari游戏（可能需要一些小调整）。

**PTAN提供了下面的实体：**
▪  Agent：知道如何将一批观察转换成一批需要执行的动作的类。它还可以包含可选状态，当需要在一个片段中为后续动作记录一些信息的时候可以用到。（我们会在第17章用到这个方法，在深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG）中，需要在探索的时候包含奥恩斯坦–乌伦贝克随机过程）。本库提供了好几个智能体用于最常见的一些RL场景，你也完全可以编写自己的BaseAgent子类。
▪  ActionSelector：一小段与Agent协同工作的逻辑，它知道如何从网络的输出中选择动作。
▪  ExperienceSource和它的变体：Agent的实例和Gym环境对象可以提供关于片段轨迹的信息。它最简单的形式就是每次一个（a, r, s'）状态转移，但其功能远不止如此。
▪  ExperienceSourceBuffer和它的变体：具有各种特性的回放缓冲区。包含一个简单的回放缓冲区和两个版本的带优先级的回放缓冲区。
▪  各种工具类，比如TargetNet和用于时间序列预处理的包装器（用于在TensorBoard中追踪训练进度）。
▪  PyTorch Ignite帮助类可以将PTAN集成到Ignite框架中去。
▪  Gym环境包装器，例如Atari游戏的包装器（从OpenAI Baselines复制而来，并做了一些调整）。

**动作选择器**
用PTAN的术语来说，动作选择器是可以帮忙将网络的输出转换成具体动作值的对象。最常见的场景包括：
▪  argmax：常被用在Q值方法中，也就是当用神经网络预测一组动作的Q值并需要一个Q(s, a)最大的动作时。
▪  基于策略的：网络的输出是概率分布（以logits的形式或归一化分布的形式），并且动作需要从这个分布采样。第4章已提到过这种情况，也就是讨论交叉熵方法的时候。
动作选择器会被Agent使用，基本上不需要自定义（当然你有权利自定义）。库中提供了几个具体类：
▪  ArgmaxActionSelector：对传入张量的第二维执行argmax。（它假设参数是一个矩阵，并且它的第一维为批维度。）
▪  ProbabilityActionSeletor：从离散动作集的概率分布中采样。
▪  EpsilonGreedyActionSelector：具有epsilon参数，用来指定选择随机动作的概率。

**在实际问题中**，**通常需要定制智能体**。原因包括：
▪  NN的架构很酷炫，它的动作空间可以同时包含连续和离散值，它可以包含多种观察（例如，文本和像素）或类似的东西。
▪  你可能想要使用非标准的探索策略，例如奥恩斯坦–乌伦贝克过程（在连续控制领域非常流行的探索策略）。
▪  你有POMDP环境，智能体的动作不是完全根据观察来决定的，而会适当包含一些智能体内部的状态（奥恩斯坦–乌伦贝克探索也是如此）。

**经验源类通过使用智能体实例和环境实例提供轨迹的每一步数据**。这些类的功能包括：
▪  支持多个环境同时交互。通过让智能体一次处理一批观察来高效地利用GPU。
▪  预处理轨迹，并以对之后训练有利的方式来表示。例如，实现一个带累积奖励的子轨迹rollout的方法。当我们不关心子轨迹的各个中间步时，可以将其删除，这样的预处理对DQN和n步DQN都很方便。它节约了内存并减少了需要编写的代码量。
▪  支持来自OpenAI Universe的向量化环境。我们会在第17章Web自动化和MiniWoB环境中介绍它。

**系统提供了三个类**：
▪  ExperienceSource：使用智能体和一组环境，它可以产生带所有中间步的n步子轨迹。
▪  ExperienceSourceFirstLast：和ExperienceSource一样，只不过将完整的子轨迹（带所有中间步）替换成了只带第一和最后一步的子轨迹，同时会将中间的奖励累积起来。这样可以节约很多内存，在n步DQN或advantage actor-critic（A2C）rollout中就会用到。
▪  ExperienceSourceRollouts：遵循Mnih关于Atari游戏的论文中描述的asynchronous advantage actor-critic（A3C）rollout方案（参见第12章）。

