# 2022.5.25

### 1.RL和其他ML方法（监督学习（supervised learning）和非监督学习（unsupervised learning））的关联和区别。

##### Sl:它的基本问题是，当给定一系列带标签的数据时，如何自动构建一个函数来将某些输入映射成另外一些输出。

​	▪  文本分类：电子邮件是否是垃圾邮件？
​	▪  图像分类和目标检测：图片包含了猫还是狗还是其他东西？
​	▪  回归问题：根据气象传感器的信息判断明天的天气。
​	▪  情感分析：某份评价反应的客户满意度是多少

##### usl:另外一个极端就是所谓的非监督学习，它假设我们的数据没有已知的标签。它的主要目标是从当前的数据集中学习一些隐藏的结构。

##### Rl:RL则处于第三阵营，介于完全监督和完全没有预定义标签之间。

### 2.RL有哪些主要形式，它们之间的关系是什么样的。

RL实体（智能体和环境）以及它们之间的交互通道（动作、奖励和观察）。

实体通过观察环境做出动作，最后获得奖励，

奖励的目的是告诉智能体它有多成功，这是RL最核心的东西。强化（reinforcement）这个术语就出自此，即智能体获得的奖励应该正向或反向地强化它的行为。奖励是局部的，意味着它反映了智能体最近的行为有多成功，而不是从开始到现在累计的行为有多成功。

智能体是通过执行确定的动作、进行观察、获得最终的奖励来和环境交互的人或物。在大多数实际RL场景中，智能体是某种软件的一部分，被期望以一种比较有效的方法来解决某个问题。

对环境的观察形成了智能体的第二个信息渠道（第一个信息渠道是奖励）。你可能会奇怪为什么我们需要这个单独的数据源。答案是方便。观察是环境为智能体提供的信息，它能说明智能体周围的情况。

### 3.RL的理论基础——马尔可夫决策过程。

马尔可夫决策过程（Markov Decision Process，MDP），用俄罗斯套娃的方式来描述它：从最简单的马尔可夫过程（Markov Process，MP）开始，然后将其扩展成马尔可夫奖励过程（Markov reward process），最后加入动作的概念，得到MDP。

MP的正式定义如下：
	▪  一组状态（S），系统可以处于任一状态。
	▪  一个转移矩阵（T），通过转移概率定义了系统的动态。

马尔可夫性质暗示了稳定性（即所有状态的底层转移概率分布不会随着时间变化）。非稳定性意味着有一些隐藏的因素在影响系统的动态，而这些因素没有被包含在观察中。但是，这与马尔可夫性质相矛盾，后者要求同一状态的底层概率分布必须相同，和状态的转移历史无关。

对于每一个片段，t时刻的回报定义如下：
￼![图片](/Users/zhuguiwei/Downloads/图片.png)
试着理解一下这个公式。对每个时间点来说，回报都是这个时间点后续得到的奖励总和，但是越远的奖励会乘越多的折扣因子，和t差的步数就是折扣因子的幂。折扣因子代表了智能体的远见性。如果γ是1，则回报Gt就是所有后续奖励的总和，对应的智能体会依赖后续所有的奖励来做出判断。如果γ等于0，则回报Gt就是立即奖励，不考虑后续任何状态，对应完全短视的智能体。

MP和马尔可夫奖励过程的转移矩阵是方阵，用行表示源状态，列表示目标状态。

策略最简单的定义是一组控制智能体行为的规则。