# 2022.6.8 rl-15

### TextWorld环境

▪  简要介绍文字冒险游戏的历史。
▪  研究TextWorld环境。
▪  实现简单的基线深度Q-network（DQN）方法，然后尝试通过使用RNN实现命令生成器来对其进行改进。这将很好地说明RL如何应用于具有庞大观察空间的复杂环境。

TextWorld的项目可在GitHub（https://github.com/microsoft/TextWorld）上找到，该项目提供了以下功能：
▪  用于文本游戏的Gym环境。它支持两种格式的游戏：Z-machine字节码（支持1到8版本）和Glulx游戏。
▪  游戏生成器，可让你根据对象的数量、描述的复杂性和任务长度等要求来随机生成任务。
▪  通过选择可查看的游戏状态来调整（针对生成的游戏）环境的复杂性。例如，可以启用中间奖励，每当智能体朝正确方向迈出一步时，中间奖励就会给智能体一个正向的奖励。下一节将详细介绍。
▪  各种实用功能，用于处理观察和动作空间、生成的游戏等。

这四个输入序列（就是我们词汇表中的token ID列表）将通过embedding层传递，然后喂给各自的LSTM RNN。我们在第14章中讨论了embedding和RNN，因此，如果你不熟悉这些NLP概念，可以查看相应部分。
LSTM网络（在图中称为“编码器”）的目标是将变长序列转换为固定大小的向量。每个输入端都经过自己的带权重的LSTM处理，这将允许网络从不同的输入序列捕获不同的数据。
编码器的输出被合并成一个向量，并传给主DQN网络。由于我们将变长序列转换为了固定大小的向量，因此DQN网络很简单：只有几个前馈层并产生单个Q值。这在计算上效率较低，但是对于基线来说已经足够了。

▪  train_basic.py：基线训练程序。
▪  lib/common.py：用于设置Ignite引擎和超参数的公用工具。
▪  lib/preproc.py：预处理管道，包括embedding和编码器类。
▪  lib/model.py：DQN模型和DQN智能体以及它们的帮助函数。

示例的训练将分两步完成：
第一步，我们将使用游戏状态和环境提供的可用命令对语言模型和预处理器进行预训练。它的训练目标为经典的交叉熵损失，也就是由命令生成器生成的命令与真实的可允许命令列表之间的交叉熵损失。当可以生成游戏可理解的命令时，我们将进入下一个训练步骤。
第二步，我们将冻结预处理器和命令生成器网络的权重，并训练命令编码器和DQN网络。

