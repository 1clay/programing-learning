# 2022.5.26 rl-2

## OpenAI Gym

### 1.将智能体插入RL框架所需的高层次要求。

▪  智能体：主动行动的人或物。实际上，智能体只是实现了某些策略的代码片段而已。这个策略根据观察决定每一个时间点执行什么动作。
▪  环境：某些世界的模型，它在智能体外部，负责提供观察并给予奖励。而且环境会根据智能体的动作改变自己的状态。

### 2.基本、纯Python实现的随机RL智能体。

▪  NumPy：用于科学计算的库，它实现了矩阵运算和常用功能。
▪  OpenCV Python bindings：计算机视觉库，提供了许多图像处理的函数。
▪  Gym：RL框架，以统一的交互方式提供了各种各样的环境。
▪  PyTorch：灵活且有表现力的深度学习（Deep Learning，DL）库。第3章会提供它的速成课。
▪  PyTorch Ignite：基于PyTorch的高级工具库，用于减少样板代码。在第3章会有简短的介绍。完整的文档参见https://pytorch.org/ignite/。
▪  PTAN（https://github.com/Shmuma/ptan）：笔者创建的一个Gym的扩展开源软件，用来支持深度RL方法以及方便地创建构造块。所有用到的类将同源代码一起详细解释。

### 3.OpenAI Gym。

**Gym的主要目的是使用统一的接口来提供丰富的RL环境。**

▪  在环境中允许执行的一系列动作。Gym同时支持离散动作和连续动作，以及它们的组合。
▪  环境给智能体提供的观察的形状[1]和边界。
▪  用来执行动作的step方法，它会返回当前的观察、奖励以及片段是否结束的指示。
▪  reset方法会将环境初始化成最初状态并返回第一个观察。

**在Gym中环境用Env类表示，它包含下面这些成员：**

▪  action_space：Space类的一个字段，限定了环境中允许执行的动作。
▪  observation_space：也是Space类的一个字段，但是限定了环境中允许出现的观察。
▪  reset()：将环境重置到初始状态，返回一个初始观察的向量。
▪  step()：这个方法允许智能体执行动作，并返回动作结果的信息——下一个观察、立即奖励以及片段是否结束的标记。这个方法有一点复杂，我们会在本节后面详细讨论。



Wrapper类继承自Env类。它的构造函数只有一个参数，即要被“包装”的Env类的实例。为了附加额外的功能，需要重新定义想扩展的方法，例如step()或reset()。唯一的要求就是需要调用超类中的原始方法。
为了处理更多特定的要求，例如Wrapper类只想要处理环境返回的观察或只处理动作，那么用Wrapper的子类过滤特定的信息即可。它们分别是：
▪  ObservationWrapper：需要重新定义父类的observation(obs)方法。obs参数是被包装的环境给出的观察，这个方法需要返回给予智能体的观察。
▪  RewardWrapper：它暴露了一个reward(rew)方法，可以修改给予智能体的奖励值。
▪  ActionWrapper：需要覆盖action(act)方法，它能修改智能体传给被包装环境的动作。



另一个应该注意的类是Monitor。它的实现方式与Wrapper类似，可以将智能体的性能信息写入文件，也可以选择将智能体的动作录下来。



