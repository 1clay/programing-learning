# 2022.5.30 rl-6

## 深度Q-network

▪  讨论价值迭代方法的问题，并考虑其名为Q-learning的变体。
▪  将Q-learning应用于所谓的网格世界环境，称为表格Q-learning。
▪  结合神经网络（Neural Network, NN）讨论Q-learning。这个组合的名称为深度Q-network（DQN）。

在每步中，价值迭代方法会对所有状态进行循环，并且对于每个状态，它都会根据Bellman近似值来更新价值。同一方法中Q值（动作价值）的变化几乎相同，但是要估算并存储每个状态和动作的价值。所以，这个过程有什么问题呢

首先，真的需要遍历状态空间中的每个状态吗？我们有一个环境，该环境可以用作真实状态样本的来源。如果状态空间中的一些状态没有展示出来，我们为什么要关心这些状态的价值呢？我们可以用从环境中获得的状态来更新状态价值，这可以节省很多工作。
如前所述，这种价值迭代的更新方法称为Q-learning，对于有明确的状态价值映射的情况，它具有以下步骤：
1）从空表开始，将状态映射到动作价值。
2）通过与环境交互，获得元组（s, a, r, s'）（状态、动作、奖励和新状态）。在此步骤中，要确定所需采取的动作，并且没有单一的正确方法来做出此决定。在第1章中，我们探讨了探索与利用的问题。本章将进行详细讨论。
3）使用Bellman近似更新Q（s, a）值：
￼![图片 2](/Users/zhuguiwei/Downloads/图片 2.png)
4)从步骤2开始重复。

DQN**训练的最终形式**

原始论文（无目标网络）已于2013年底发表（“Playing Atari with Deep Reinforcement Learning”, 1312.5602v1, Mnih等），并使用了7款游戏进行测试。后来，在2015年初，该论文的修订版包含了49种不同的游戏，发表在Nature杂志上（“Human-Level Control Through Deep Reinforcement Learning”, doi:10.1038/nature14236, Mnih等）。
前面论文中的DQN算法包含以下步骤：
1）使用随机权重（ε←1.0）初始化Q(s, a)和￼的参数，清空回放缓冲区。
2）以概率ε选择一个随机动作a，否则￼。
3）在模拟器中执行动作a，观察奖励r和下一个状态s'。
4）将转移过程(s, a, r, s')存储在回放缓冲区中。
5）从回放缓冲区中采样一个随机的小批量转移过程。
6）对于回放缓冲区中的每个转移过程，如果片段在此步结束，则计算目标y=r，否则计算￼。
7）计算损失：L=(Q(s, a)–y)2。
8）通过最小化模型参数的损失，使用SGD算法更新Q(s, a)。
9）每N步，将权重从Q复制到￼。
10）从步骤2开始重复，直到收敛为止。

#### DQN模型

在Nature杂志上发表的模型有三个卷积层，然后是两个全连接层。所有层均由线性整流函数（Rectified Linear Unit, ReLU）非线性分开。模型的输出是环境中每个动作的Q值，没有应用非线性（因为Q值可以有任何值）。与逐个处理Q(s, a)并将观察值和动作反馈到网络以获得动作价值相比，通过网络一次计算所有Q值的方法有助于显著提高速度。

在具有较大观察空间的复杂环境中进行价值迭代的局限性，如何通过Q-learning来克服它们。在FrozenLake环境中验证了Q-learning算法，讨论了用NN进行Q值的近似以及由此近似所带来的额外复杂性。
DQN改善其训练稳定性和收敛性的技巧，例如经验回放缓冲区、目标网络和帧堆叠。最后，将这些扩展组合到DQN的实现中，解决了Atari游戏中的Pong环境。

