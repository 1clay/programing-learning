# 2022.6.1 rl-8

### DQN扩展

▪  N步DQN：如何通过简单展开Bellman方程来提升收敛速度和稳定性，以及为什么它不是最终方案。
▪  Double DQN：如何处理DQN对动作价值评估过高的问题。
▪  噪声网络：如何通过增加网络权重的噪声来提升探索的效率。
▪  带优先级的回放缓冲区：为什么对经验进行均匀采样不是训练的最佳方法。
▪  Dueling DQN：如何通过使网络结构更接近正在解决的问题来加速收敛。
▪  Categorical DQN：如何跳脱动作的单个期待价值，使用完整的分布。

**DQN与Qleanring类似都是基于值迭代的算法，但是在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，而当状态和动作空间是高维连续时，使用Q-Table不动作空间和状态太大十分困难。**
所以在此处可以把Q-table更新转化为一函数拟合问题，通过拟合一个函数function来代替Q-table产生Q值，使得相近的状态得到相近的输出动作。因此我们可以想到深度神经网络对复杂特征的提取有很好效果，所以可以将DeepLearning与Reinforcement Learning结合。这就成为了DQN

DL与RL结合存在以下问题 ：

DL是监督学习需要学习训练集，强化学习不需要训练集只通过环境进行返回奖励值reward，同时也存在着噪声和延迟的问题，所以存在很多状态state的reward值都是0也就是样本稀疏
DL每个样本之间互相独立，而RL当前状态的状态值是依赖后面的状态返回值的。
当我们使用非线性网络来表示值函数的时候可能出现不稳定的问题
DQN中的两大利器解决了以上问题

通过Q-Learning使用reward来构造标签
通过experience replay（经验池）的方法来解决相关性及非静态分布问题
使用一个MainNet产生当前Q值，使用另外一个Target产生Target Q

**基础DQN的实现中，有三个模块：**
▪  Chapter08/lib/dqn_model.py：DQN神经网络，代码和第6章中的一样，所以这里不再赘述。
▪  Chapter08/lib/common.py：本章其他代码会用到的通用函数和声明。
▪  Chapter08/01_dqn_basic.py：60行使用了PTAN和Ignite库的代码，实现了基础DQN方法。

在训练过程中学习探索特征，而不是单独定制探索的策略。
基础DQN通过选择随机动作来完成探索，随机选取会依据特定的超参数epsilon，它会随着时间的增长慢慢地从1.0（完全随机选择动作）降至一个小比例0.1或0.02。这个流程适用于片段较短的简单环境，在游戏过程中不会有太多不稳定的情况，但是即使是在这样简单的情况下，它也需要调优参数来让训练更高效。
在“Noisy Networks for Exploration”论文中，作者提出了一个非常简单的解决方案，但是效果很好。他们在全连接层中加入噪声，并通过反向传播在训练过程中调整噪声参数。当然了，不要将这个方法和“用网络来决定在哪里进行探索”相混淆，后者是一种更为复杂的方法，也得到了广泛的支持（例如，请参见关于内在动机和基于计数的探索方法[5-6]）。我们会在第21章讨论高级探索技术。
论文作者提出了两种添加噪声的方法，根据他们的实验这两种方法都有效，但是它们具有不同的计算开销：
1）独立高斯噪声：对于全连接层中的每个权重，都加入一个从正态分布中获取的随机值。噪声的参数μ和σ被存在层中并使用反向传播训练，如同我们训练标准的线性层中的权重一样。以与线性层相同的方式计算这种“噪声层”的输出。
2）分解高斯噪声：为了最小化采样的随机数数量，作者提出只保留两个随机向量：一个和输入的大小一样，另一个和层的输出大小一样。然后，通过计算向量的外积来创建该层的一个随机矩阵。

**根据训练损失对样本进行优先级排序，以提升从回放缓冲区中采样的效率。**

populate()方法需要从ExperienceSource对象中获取指定数量的状态转移并将其存储在缓冲区中。由于状态转移的存储是用循环缓冲区来实现的，在缓冲区中有两种情况：
▪  缓冲区没有达到最大的容量限制时，只需要将新的状态转移附加到缓冲区中。
▪  缓冲区已满时，需要将通过pos字段跟踪的旧状态转移覆盖，并通过取模缓冲区大小来调整这个位置。

**其中的几点改变：**
▪  现在批包含3个实体：批数据、所采样项的索引，以及样本的权重。
▪  调用了新创建的损失函数，它以权重为参数并返回额外的条目优先级。它们被传给了buffer.update_priorities函数用以重新划分所采样的样本优先级。
▪  调用缓冲区的update_beta方法来按计划改变β参数。

`Q-Learning`是强化学习算法中**value-based**的算法，Q即为Q（s，a），就是在某一个时刻的`state`状态下，采取动作a能够获得收益的期望，环境会根据`agent`的动作反馈相应的`reward`奖赏，所以算法的主要思想就是将`state`和`action`构建成一张`Q_table`表来存储Q值，然后根据Q值来选取能够获得最大收益的动作。

Q-learning的主要优势就是使用了**时间差分法（融合了蒙特卡洛和动态规划）能够进行off-policy**的学习，使用**贝尔曼方程**可以对**马尔科夫过程**求解最优策略。

定义神经网络结构以及对结果有帮助的方法：
▪  Dueling DQN：神经网络会被分成两个路径，分别是状态价值的概率分布和优势值的概率分布。输出中，两个路径的值会被加在一起，并提供动作的最终概率分布值。为了将优势值函数的平均值限制为0，让每个原子的概率分布减去平均优势值。
▪  噪声网络：在价值和优势值路径的线性层使用nn.Linear的噪声版本。

