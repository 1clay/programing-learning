# 2022.6.10 rl-17

### 连续动作空间 

▪  涵盖连续动作空间为何重要、它与离散动作空间有何不同，以及在Gym API中的实现方式。
▪  讨论将RL方法应用到连续控制的领域。
▪  查看解决四足机器人问题的三种不同算法。

critic的forward()函数首先用其较小的网络对观察进行转换，然后将输出和给定的动作进行组合，以将其转换为单个Q值。若要将actor神经网络与PTAN经验源一起使用，我们需要定义一个将观察结果转化为动作的智能体类。此类是放置OU探索过程最方便的地方，但是要更方便地执行此操作，我们还应该使用PTAN智能体的一个功能：可选状态。

最显著的变化是critic的输出。现在，它不返回给定状态和动作的单个Q值，而是返回N_ATOMS个值，该值对应于预定义范围中的值的概率。在我的代码中，我使用了N_ATOMS=51且分布范围为Vmin=-10和Vmax=10，因此critic返回51个数字，表示带折扣的奖励会落入[–10, –9.6, –9.2, …, 9.6, 10]中。
D4PG和DDPG之间的另一个区别是探索。DDPG使用OU过程进行探索，但据D4PG的作者称，他们同时尝试了在动作中添加OU以及简单的随机噪声，结果是相同的。因此，他们在本文中使用了一种更简单的方法进行探索。
代码中的最后一个显著差异与训练有关，因为D4PG使用交叉熵损失来计算两个概率分布之间的差异（由critic返回并由Bellman运算符得出）。为了使两个分布的原子对齐，与Bellemare在原始论文中的使用方式相同，他们使用了分布投影。

**可以尝试的事情**

1）在D4PG代码中，我使用了一个简单的回放缓冲区，它足以对DDPG进行良好的改进。你可以尝试按照与第8章中相同的方式将示例切换到带优先级的回放缓冲区，然后检查效果。
2）还有许多有趣且充满挑战的环境。例如，你可以从其他PyBullet环境开始，但也可以使用DeepMind Control Suite（见Tassa、Yuval等人发表的“DeepMind Control Suite”，arXiv abs/1801.00690（2018））、Gym中基于MuJoCo的环境以及许多其他环境。
3）你可以申请MuJoCo的试用许可证，并将其稳定性、性能和产生的策略与PyBullet进行比较。
4）你可以参加NIPS-2017极富挑战性的Learning to Run竞赛（该竞赛也在2018年和2019年进行，并存在更多具有挑战性的问题），你将获得一个人体模拟器，并且你的智能体需要弄清楚如何移动它。