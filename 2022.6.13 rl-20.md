# 2022.6.13 rl-20

### 强化学习中的黑盒优化

▪  进化策略。
▪  遗传算法。

黑盒方法具有几个非常吸引人的属性：
▪  它们比基于梯度的方法至少快两倍，因为我们不需要执行反向传播步骤来获得梯度。
▪  不会对优化的目标和被视为黑盒的策略做太多假设。当奖励函数不平滑或策略包含随机选择的步骤时，传统方法会遇到困难。对于黑盒方法而言，所有这些都不是问题，因为它们对黑盒内部并不期望太多。
▪  这些方法通常可以很好地并行化。例如，上述的随机搜索可以轻松扩展到让数千个CPU或GPU并行工作，并且彼此之间没有任何依赖。对于DQN或策略梯度方法情况就不一样了，因为你需要累积梯度并将当前策略传播到所有并行的worker，而这会降低并行度。
它的缺点通常是较低的采样效率。尤其是对策略进行单纯的随机搜索时，策略是由具有几十万个参数的神经网络进行参数化的，那么搜索成功的可能性将非常低。

**进化策略**

. 初始化学习速率α、噪声标准差σ和初始值策略参数θ0。
2. 对于t = 0, 1, 2, …，执行：
  1）采样带有权重形状的噪声样本：ε1, …, εn～N(0, 1)。
  2）计算i = 1, …, n时的返回值Fi = F(θt + σεi)。
  3）更新权重￼。

  

  GA方法的步骤：
  1. 初始化突变力量σ、人群总数N，要选择的个体数T和初始的人群P0，以及N个随机初始化的策略及其适应度：￼。
  2. 对于g = 1…G：
  1）对Pg–1按照适应度函数值Fg–1降序排序。
  2）复制精英￼。
  3）对于个体i = 2…N：
  ▪  k =从1…T中随机选择的父对象。
  ▪  采样εn～N(0, 1)。
  ▪  突变父对象：￼。
  ▪  获得它的适应度：￼。

对基本GA方法的另一种优化是新颖性搜索（NS），由Lehman和Stanley在2011年发表的论文“Lehman and Stanley in their paper, Abandoning Objectives: Evolution through the Search for Novelty Alone”[3]中提出。
NS的想法是改变我们优化的目标。我们不再试图增加来自环境的总奖励，而是奖励智能体探索其从未检查过的行为（即新颖的行为）。根据作者对存在许多陷阱的迷宫导航问题的实验，NS比其他奖励驱动的方法要好得多。
为了实现NS，我们定义了行为特征（BC）（π），它描述了策略的行为以及两个BC之间的距离。然后，我们使用k近邻方法检查新策略的新颖性，并根据该距离来驱动GA。在“Deep Neuroevolution”论文[2]中，需要智能体进行充分的探索。此时NS方法明显优于ES、GA和其他更传统的针对RL问题的方法。

