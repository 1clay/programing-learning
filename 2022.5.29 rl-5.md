# 2022.5.29 rl-5

## 表格学习和Bellman方程

▪  查看状态的价值和动作的价值，并学习如何在简单的情况下进行计算。
▪  讨论Bellman方程，以及在知道价值的情况下如何建立最佳策略。
▪  讨论价值迭代方法，然后在FrozenLake环境中进行尝试。
▪  对Q-learning方法做同样的事情。

状态的最优价值等于动作所获得最大预期的立即奖励，再加上下一状态的长期折扣奖励。你可能还会注意到，这个定义是递归的：状态的价值是通过立即可到达状态的价值来定义的。这种递归可能看起来像作弊：我们定义一些值，并假装它们是已知的。但是，这却是计算机科学甚至数学中非常强大且通用的技术（归纳证明也是基于这样的技巧）。Bellman方程不仅是RL的基础，还是更通用的动态规划（解决实际优化问题的广泛使用的方法）的基础。

#### 价值迭代实践

▪  奖励表：带有复合键“源状态”+“动作”+“目标状态”的字典。该值是从立即奖励中获得的。
▪  转移表：记录了各转移的次数的字典。键是复合的“状态”+“动作”，而值则是另一个字典，是所观察到的目标状态和次数的映射。例如，如果在状态0中，执行动作1十次，其中有三次导致进入状态4，七次导致进入状态5。该表中带有键（0, 1）的条目将是一个字典，内容为{4：3,5：7}。我们可以使用此表来估计转移概率。
▪  价值表：将状态映射到计算出的该状态的价值的字典

