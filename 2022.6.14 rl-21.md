# 2022.6.14 rl-21

### 高级探索

▪  讨论为什么探索是RL中如此重要的主题。
▪  探索ε-greedy方法的效力。
▪  了解替代方案，并在不同的环境中进行尝试。

在理论RL中，对此存在严格的定义，但是宏观思想很简单直观。当我们不在以下状态浪费时间时，探索是有效的：智能体已经看过并熟悉的环境。智能体不应一遍又一遍地执行相同动作，而需要寻找全新的经验。正如我们已经讨论过的，探索必须和利用相平衡，它们是相反的，利用意味着使用我们的知识以最有效的方式获得最佳奖励。现在让我们快速讨论为什么我们会对有效探索感兴趣。

高层面来看，两种方法做的事一样：探索环境，将随机性引入动作。但是，最近的研究表明，这种方法远非理想做法：
▪  对于价值迭代方法，在我们的某些轨迹中执行的随机动作会将偏差引入Q值估计中。Bellman方程假设下一个状态的Q值是从Q值最大的动作中获得的。换句话说，轨迹的剩余部分应该来自最优行为。但是，使用ε-greedy时，我们可能不会执行最优动作，而只是执行随机动作，并且这条轨迹将在回放缓冲区中存储很长一段时间，直到ε被衰减并且旧样本从缓冲区中被剔除为止。在此之前，我们将学习错误的Q值。
▪  随着随机动作注入轨迹，我们的策略在每一步都会变化。根据ε值或熵损失系数定义的频率，我们的轨迹会不断在随机策略和当前策略之间连续切换。在需要执行多个步骤才能到达环境状态空间中某些孤立区域的情况下，这可能会导致状态空间覆盖不足。

 三种不同的探索方法：
▪  策略中的随机性，我们将随机性添加到用于获取样本的策略中。这个系列中的方法是噪声网络，我们已经讨论过了。
▪  基于计数的方法，该方法可跟踪智能体看过特定状态的次数。我们将研究两种方法：状态的直接计数方法和状态的伪计数方法。
▪  基于预测的方法，该方法尝试根据状态和预测质量来预测某些内容。我们可以判断智能体对这种状态的熟悉程度。为了说明这种方法，我们将看一下策略蒸馏方法，该方法显示了在探索困难的Atari游戏（如蒙特祖玛的复仇）中的最先进的结果。

