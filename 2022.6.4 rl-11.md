# 2022.6.4 rl-11

### 策略梯度：一种替代方法

▪  与已经熟悉的Q-learning进行对比，概述该方法的动机、优势和劣势。
▪  从被称为REINFORCE的简单策略梯度方法开始，尝试将其应用于CartPole环境，并将其与深度Q-network（DQN）方法进行比较。

交叉熵方法即使使用如此简单的假设也可以工作，但是使用Q(s,a)而不是仅使用0和1进行训练会有明显的提升。因为进行了更细粒度的片段分离。例如，比起奖励是1的片段中的状态转移，总奖励为10的片段的状态转移应该对梯度有更多的贡献。使用Q(s,a)而不是仅使用0或1常数的第二个原因，增加片段开始时优质动作的概率，并减少更接近片段结尾的动作（因为Q(s,a)包含折扣因子，所以会自动考虑较长动作序列的不确定性）。这是REINFORCE方法的思想。其步骤如下：
1）用随机权重初始化网络。
2）运行N个完整的片段，保存其(s,a,r,s')状态转移。
3）对于每个片段k的每一步t，计算后续步的带折扣的总奖励：￼。
4）计算所有状态转移的损失函数：￼。
5）执行SGD更新权重，以最小化损失。
6）从步骤2开始重复，直到收敛。
**上述算法在几个重要方面与Q-learning不同：**
▪  不需要显式的探索。在Q-learning中，使用ε-greedy策略来探索环境，并防止智能体陷入非最优策略的困境。现在，利用神经网络返回的概率，可以实现自动探索。在开始时，使用随机权重初始化神经网络，它会返回均匀的概率分布。此分布对应于智能体的随机行为。
▪  不需要使用回放缓冲区。策略梯度方法属于在线策略方法，这意味着我们无法用旧策略获得的数据来训练。它有优点，也有缺点。优点是这些方法通常收敛更快。缺点是，与诸如DQN之类的离线策略方法相比，它们通常需要与环境进行更多的交互。
▪  不需要目标网络。我们在这里使用Q值，但是它们是根据我们从环境中得到的经验获得的。在DQN中，我们使用目标网络打破了Q值近似时的相关性，但是我们现在不再进行近似了。下一章将展示目标网络技巧在策略梯度方法中仍然有用。

**基于策略的方法和基于价值的方法**

▪  策略方法可以直接优化我们关心的内容：行为。诸如DQN之类的价值方法间接地实现这一点，首先学习价值，然后根据此价值提供策略。
▪  策略方法是在线策略方法，需要从环境获取新鲜样本。价值方法可以从来自旧策略、人工制造和其他来源的旧数据受益。
▪  策略方法的采样效率通常较低，这意味着它们需要与环境进行更多交互。价值方法可以受益于较大的回放缓冲区。但是，采样效率高并不意味着价值方法的计算效率也更高，情况通常恰恰相反。
▪  在前面的示例中，在训练过程中，我们只需访问一次NN，即可获得动作的概率。而在DQN中，我们需要处理两批状态：一次是当前状态，一次是Bellman更新中的下一个状态。

在信息论中，熵是某些系统中不确定性的度量。将熵应用到智能体的策略中，它可以显示智能体对执行何种动作的不确定程度。策略的熵可以用数学符号定义为：H(π) = –∑π(a|s)logπ(a|s)。熵的值始终大于零，并且在策略符合平均分布（换句话说，所有动作具有相同的概率）时具有一个最大值。当策略决定某个动作的概率为1而所有其他动作的概率为0时，熵就变得最小，这意味着该智能体完全确定要做什么。为了防止智能体陷入局部最小值，在损失函数中减去熵，以惩罚智能体过于确定要采取的动作。

