# 2022.6.7 rl-14

### 使用强化学习训练聊天机器人

▪  简要介绍NLP基础知识，包括循环神经网络（Recurrent Neural Network，RNN）、词嵌入（word embedding）和seq2seq（序列到序列）模型。
▪  讨论NLP和RL问题之间的相似性。
▪  了解有关如何使用RL方法改进NLP seq2seq训练的最初想法。

现代DL驱动的NLP的另一个标准构建块是词嵌入（也被称为word2vec），它是最流行的训练方法之一。这个想法始于在NN中表示我们的语言序列。通常，NN使用固定大小的数字向量，但是在NLP中，我们通常使用单词或字符作为模型的输入。

两种方法获得这种映射。首先，你可以下载所需语言的预训练向量。具体可以在Google上搜索GloVe预训练向量或word2vec预训练向量（GloVe和word2vec是用于训练此类向量的不同方法，它们会产生相似的结果）。
第二种方法是在你自己的数据集上训练它们。为此，你可以使用特殊工具，例如fasttext（https://fasttext.cc/，Facebook的开源代码工具），或者随机初始化embedding并允许你的模型在正常训练期间对其进行调整。

**实践中，用于seq2seq训练的REINFORCE可以写为以下算法：**
1）对于数据集中的每个样本，使用RNN编码器获得编码表示的E。
2）使用特殊的开始token初始化当前token：T ='<BEG>'。
3）用空序列初始化输出序列：Out = []。
4）当T！='<END>'时：
▪  传入当前token和隐藏状态，获取输出token的概率分布和新的隐藏状态：p,H = Decoder(T, E）。
▪  从概率分布中采样输出token：Tout'。
▪  记住概率分布p。
▪  将Tout附加到输出序列：Out+=Tout。
▪  设置当前token：T←Tout, E←H。
5）计算Out和参考序列之间的BLEU或其他指标：Q=BLEU(Out, Outref)。
6）评估梯度：∇J=∑TQ∇logp(T)。
7）使用SGD更新模型。
8）重复直到收敛。

**Chapter14文件夹中，包含以下几个部分：**
▪  data：带有get_data.sh脚本的目录，用于下载和解压缩将在示例中使用的数据集。数据集的归档大小为10MB，包含从各种来源提取的结构化对话，也被称为Cornell Movie-Dialogs语料库，可从下面的网站获取：https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html。
▪  libbots：一个目录，其中包含各个示例的组件之间共享的Python模块。下一节将介绍这些模块。
▪  tests：包含用于库模块的单元测试的目录。
▪  根文件夹包含两个用于训练模型的程序：train_crossent.py（用于一开始训练模型）和train_scst.py（用于使用REINFORCE算法微调预训练的模型）。
▪  一个用于显示数据集中的各种统计信息和数据的脚本：cor_reader.py。
▪  用于将训练后的模型应用于数据集的脚本，并显示质量指标：data_test.py。
▪  针对用户提供的短语使用模型的脚本：use_model.py。
▪  针对Telegram Messenger[1]使用的机器人，会使用预先训练的模型：telegram_bot.py。

data.py模块可在更高级别上使用，并且它不包含任何特定于数据集的知识。它提供以下功能，该示例几乎在所有地方都使用了这些功能：
▪  处理token和其整数ID之间的映射工作：从文件中保存和加载数据（save_emb_dict()和load_emb_dict()），将token列表编码为ID列表（encode_words()），将整数ID列表解码成token列表（decode_words()），并根据训练数据生成映射用字典（phrase_pairs_dict()）。
▪  处理训练数据：迭代给定大小的批次（iterate_batchs()）并将数据拆分为训练/测试部分（split_train_test()）。
▪  加载对话数据并将其转换为适合训练的短语-答复对：load_data()。
在数据加载和字典创建时，我们还额外添加了一些具有预定义ID的特殊token：
▪  用于未知单词的token：#UNK，用于所有字典外token。
▪  序列开头的token：#BEG，位于所有序列之前。
▪  序列结尾的token：#END。

**将RL训练方法应用于seq2seq问题有可能改善最终模型。主要原因是：**
▪  更好地处理多个目标序列。例如，hi可以回答为hi、hello、not interested等。RL将解码器视为选择动作的程序，而每个动作都是要生成的token，这更适用于该问题。
▪  可以直接优化BLEU分数，而不是交叉熵损失。使用生成序列的BLEU分数作为梯度缩放，我们可以将模型推向成功序列，并降低不成功序列的可能性。
▪  通过重复解码过程，我们可以生成更多片段进行训练，这将得到更好的梯度估计。
▪  此外，使用self-critical序列训练方法，我们几乎可以免费获得基线，而不会增加模型的复杂性，从而进一步提高收敛性。