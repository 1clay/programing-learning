# 2022.6.5 rl-12

### actor-critic方法

▪  探索基线如何影响统计数据和梯度的收敛。
▪  涵盖基线概念的一种扩展。

**advantage actor-critic方法，通常被简称为A2C**

训练角度来看，会执行以下步骤：
1）使用随机值初始化网络参数θ。
2）使用当前策略πθ在环境中交互N步，并保存状态（st）、动作（at）和奖励（rt）。
3）如果片段到达结尾，则R=0，否则为Vθ(st)。
4）对于i=t–1…tstart（请注意是从后向前处理的）：
◦  R←ri + γR。
◦  累积策略梯度：￼。
◦  累积价值梯度：￼。
5）使用累积梯度更新网络参数，使其沿着策略梯度（∂θπ）的方向移动，以及沿着价值梯度（∂θv）的反方向移动。
6）从步骤2开始重复，直到收敛。
前面的算法是一个概述，与通常在研究论文中描述的算法类似。实践中，还有如下一些注意事项：
▪  通常会加入entropy bonus以改善探索。通常将其写成加入损失函数的熵值：LH = β∑iπθ(si)logπθ(si)。当概率分布是均匀分布时，该函数达到最小值，因此通过将其添加到损失函数中，我们可以使智能体避免过于确定其动作。
▪  梯度累积通常被实现为结合了所有三个组件（策略损失、价值损失和熵损失）的一个损失函数。应该谨慎对待这些损失的符号，因为策略梯度会显示策略改进的方向，但应将价值损失和熵损失都降至最低。
▪  为了提高稳定性，需要使用多个环境，它们同时向你提供观察结果（当有多个环境时，将使用它们的观察结果创建训练批）。下一章将介绍几种方法来做这件事。
前面所述的使用多个并行环境的版本称为advantage asynchronous actor-critic，也称为A3C。

从原始超参开始，并执行以下实验：
▪  提高学习率。
▪  增加熵的beta值。
▪  更改用来收集经验的环境的数量。
▪  调整批大小。
严格来说，以下实验并不是正确的超参调优方式，它只是试图更好地了解A2C收敛动态如何依赖于其参数。为了找到最佳参数集，使用全网格搜索或随机采样可能会得到更好的结果，但是它们需要更多的时间和资源。

